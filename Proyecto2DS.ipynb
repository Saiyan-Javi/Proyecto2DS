{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "43cea831",
      "metadata": {
        "id": "43cea831"
      },
      "source": [
        "# Proyecto 2\n",
        "## Data Science, sección 40\n",
        "## Grupo 1\n",
        "Javier Alejandro Ovalle Chiquín, 22103  \n",
        "José Ángel Morales Farfan, 22689  \n",
        "Ricardo Josué Morales Contreras, 22289  \n",
        "Karen Daniela Pineda Ventura 231132"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39edd5c5",
      "metadata": {
        "id": "39edd5c5"
      },
      "source": [
        "## Planteamiento inicial del problema"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ccb9aa",
      "metadata": {
        "id": "32ccb9aa"
      },
      "source": [
        "### Situación problemática\n",
        "Los estudiantes realizan resúmenes en respuesta a una consigna, luego evaluadores puntúan cada resumen en dos factores: content (Si el resumen cubre lo solicitado) y wording (calidad de la redacción). Evaluar manualmente esto es costoso, lento y potencialmente inconsistente. Por lo que, automatizar la evaluación con modelos de PLN puede permitir retroalimentación a escala, detectar rápido resúmenes que necesiten intervención y homogeneizar criterios.\n",
        "\n",
        "### Problema científico\n",
        "Construir modelos de Procesamiento del Lenguaje Natural que, a partir del texto del resumen y del enunciado del prompt, predigan dos puntuaciones continuas (content y wording) de forma que se reduzca la discrepancia con respecto a las evaluaciones realizadas manualmente.\n",
        "\n",
        "### Objetivos\n",
        "\n",
        "Objetivo general\n",
        "\n",
        "- Diseñar, implementar y evaluar un pipeline de PLN que prediga las puntuaciones content y wording de resúmenes estudiantiles con una evaluación cuantitativa reproducible (RMSE, correlaciones).\n",
        "\n",
        "Objetivos específicos\n",
        "\n",
        "- Implementar preprocesamiento reproducible (tokenización, limpieza) y documentar su efecto en la calidad de features\n",
        "\n",
        "- Entrenar un baseline (TF-IDF + Ridge) validado por prompt con GroupKFold y obtener RMSE por target; comparar resultados con una línea base (por ejemplo, promedio)\n",
        "\n",
        "- Realizar EDA que identifique valores faltantes, distribución de scores, outliers y diferencias entre prompts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sU6MfMY84Dnb",
      "metadata": {
        "id": "sU6MfMY84Dnb"
      },
      "source": [
        "## Descripción de los datos\n",
        "\n",
        "Variables principales\n",
        "- student_summary: texto escrito por estudiantes\n",
        "- prompt: enunciado al que responde el resumen\n",
        "- content: puntuación otorgada al contenido\n",
        "- wording: puntuación de la redacción\n",
        "\n",
        "Operaciones de limpieza\n",
        "- Conversión a minúsculas\n",
        "- Eliminación de signos de puntuación, caracteres especiales y stopwords\n",
        "- Tokenización y lematización\n",
        "- Manejo de valores faltantes (textos vacíos se reemplazan por cadenas vacías)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bZM3uLOlCTk",
      "metadata": {
        "id": "4bZM3uLOlCTk"
      },
      "source": [
        "## Investigación preliminar\n",
        "\n",
        "**Técnicas comunes para detección de patrones de texto**\n",
        "\n",
        "**El preprocesamiento de Lenguaje Natural (PLN)** busca extraer conocimiento útil a partir de textos. En problemas como el de la competencia, el objetivo es detectar patrones en los resúmenes escritos por estudiantes que permitan estimar automáticamente la calidad de su contenido y redacción.\n",
        "\n",
        "Entre las técnicas están:\n",
        "\n",
        "Preprocesamiento de texto\n",
        "Antes de aplicar cualquier modelo, es necesario transformar el texto original para que sea más manejable:\n",
        "- Normalización: pasar todo a minúsculas, eliminar signos de puntuación, URLs y caracteres especiales. Esto disminuye el ruido y hace más consistentes los datos.\n",
        "- Tokenización: dividir el texto en unidades (tokens), que suelen ser palabras o subpalabras. Ejemplo: \"The boy runs fast\" = [\"the\", \"boy\", \"runs\", \"fast\"].\n",
        "- Stopwords  removal: eliminación de palabras vacías (the, and, is), que no aportan tanta información\n",
        "- Lematización/Stemming: reducir palabras a su forma base (\"running\" = \"run\"). Permite agrupar variaciones morfológicas\n",
        "- Correción de valores faltantes y outliers: textos muy cortos, vacíos o muy alrgos deben analizarse aparte\n",
        "\n",
        "**Representación vectorial del texto**\n",
        "\n",
        "El texto debe representarse numéricamente para que un modelo pueda procesarlo.\n",
        "\n",
        "**Bolsa de palabras (Bag of words):** Cada documento es representado como un vector de frecuencias de palabras, ignorando el orden. Es simple y útil para detectar patrones de vocabulario físico.\n",
        "\n",
        "**TF-IDF (Term Frecuency - Inverse Document Frecuency):**\n",
        "Pondera palabras según su frecuencia en un documento, penaliza las que son muy frecuentes, lo cual permite detectar palabras clave más relevantes para diferenciar textos.\n",
        "\n",
        "**N-gramas:** Considera secuencias de palabras (bigramas, trigramas). Esto captura patrones de estilo y frases típicas de resúmenes.\n",
        "\n",
        "**Embeddings semánticos:**\n",
        "- Word2Vec, Glove, FastText, representan palabras en vectores densos que capturan similitud semántica (\"dog\" y \"puppy\").\n",
        "- Embeddings contextuales (Transformers como BERT, RoBERTa, DistilBERT) capturan el significado de cada palabra en función de su contexto, lo que permite representar mejor la semántica de frases completas.\n",
        "\n",
        "Patrones estilísticos y estructurales\n",
        "Se pueden usar métricas como:\n",
        "- Longitud del texto: cantidad de caracteres o palabras\n",
        "- Riqueza léxica: proporción de palabras únicas frente al total\n",
        "- Medidas de legibilidad: índices como Flesh-Kincaid, que miden la complejidad sintáctica\n",
        "- Distribución de puntuación y longitud de oraciones: redacción más clara suele usar estructuras más balanceadas\n",
        "\n",
        "**Modelos para detección de patrones**\n",
        "\n",
        "Regresión lineal y Ridge/Lasso: adecuados como modelos base para datos vectorizados (TF-IDF).\n",
        "\n",
        "Árboles de decisión y ensambles (Random Forest, XGBoost, LightGBM): capturan patrones no lineales en los features.\n",
        "\n",
        "Redes neuronales: CNN y RNN, detectan secuencias recurrentes en textos.\n",
        "\n",
        "Transformers (BERT, RoBERTa, GPT, etc): de última generación, capaces de capturar relaciones complejas y contexto profundo, especialmente efectivos evaluación de calidad de texto.\n",
        "\n",
        "**Evaluación de patrones**\n",
        "\n",
        "Métricas cuantitativas: RMSE (Root Mean Squared Error), MAE (Mean Absolute Error) y correlaciones estadísticas.\n",
        "\n",
        "Validación cruzada estratificada por prompt: asegura que el modelo no se sobreajuste, ya que los resúmenes son de temas distintos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yjncjEDEuhEm",
      "metadata": {
        "id": "yjncjEDEuhEm"
      },
      "source": [
        "Importaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "HVWdljohwCRs",
      "metadata": {
        "id": "HVWdljohwCRs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.utils import indexable\n",
        "from sklearn.model_selection import check_cv\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GroupKFold, cross_validate\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hvylcok5wEtT",
      "metadata": {
        "id": "hvylcok5wEtT"
      },
      "source": [
        "Configuración"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ad28d5fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad28d5fc",
        "outputId": "4e3a324a-9e58-429e-854f-36fe4fa4339c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\Javier\n",
            "[nltk_data]     Chiquin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to C:\\Users\\Javier\n",
            "[nltk_data]     Chiquin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to C:\\Users\\Javier\n",
            "[nltk_data]     Chiquin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Visual config\n",
        "plt.style.use('seaborn-v0_8-deep')\n",
        "plt.rcParams['figure.figsize'] = (8,5)\n",
        "\n",
        "# Inicializar NLTK\n",
        "for resource in ['punkt', 'punkt_tab', 'stopwords']:\n",
        "    try:\n",
        "        nltk.data.find(resource)\n",
        "    except LookupError:\n",
        "        nltk.download(resource)\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# Directorio de datos\n",
        "DATA_DIR = Path('.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B4XtMelEdxs8",
      "metadata": {
        "id": "B4XtMelEdxs8"
      },
      "source": [
        "### Carga de datos y descripción"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ffRyrb8Zdz4t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffRyrb8Zdz4t",
        "outputId": "9cdb7668-26b9-4f15-b1c8-4cdc5a780fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (7165, 5)\n",
            "Test shape : (4, 3)\n",
            "Prompts train shape: (4, 4)\n",
            "Sample submission shape: (4, 3)\n",
            "\n",
            "Train columns: ['student_id', 'prompt_id', 'text', 'content', 'wording']\n",
            "\n",
            "Ejemplo de filas (train):\n",
            "      student_id prompt_id                                               text  \\\n",
            "0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n",
            "1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n",
            "2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n",
            "\n",
            "    content   wording  \n",
            "0  0.205683  0.380538  \n",
            "1 -0.548304  0.506755  \n",
            "2  3.128928  4.231226  \n"
          ]
        }
      ],
      "source": [
        "def load_data(data_dir=DATA_DIR):\n",
        "    train = pd.read_csv(data_dir / 'summaries_train.csv')\n",
        "    test  = pd.read_csv(data_dir / 'summaries_test.csv')\n",
        "    prompts_train = pd.read_csv(data_dir / 'prompts_train.csv')\n",
        "    prompts_test  = pd.read_csv(data_dir / 'prompts_test.csv')\n",
        "    sample = pd.read_csv(data_dir / 'sample_submission.csv')\n",
        "    return train, test, prompts_train, prompts_test, sample\n",
        "\n",
        "# Cargar\n",
        "train, test, prompts_train, prompts_test, sample = load_data()\n",
        "print('Train shape:', train.shape)\n",
        "print('Test shape :', test.shape)\n",
        "print('Prompts train shape:', prompts_train.shape)\n",
        "print('Sample submission shape:', sample.shape)\n",
        "print('\\nTrain columns:', train.columns.tolist())\n",
        "print('\\nEjemplo de filas (train):\\n', train.head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "nwgkbbqseEGD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwgkbbqseEGD",
        "outputId": "3599ab81-9fa8-4575-bab1-102424c382a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           content      wording\n",
            "count  7165.000000  7165.000000\n",
            "mean     -0.014853    -0.063072\n",
            "std       1.043569     1.036048\n",
            "min      -1.729859    -1.962614\n",
            "25%      -0.799545    -0.872720\n",
            "50%      -0.093814    -0.081769\n",
            "75%       0.499660     0.503833\n",
            "max       3.900326     4.310693\n",
            "\n",
            "Missing values:\n",
            " student_id    0\n",
            "prompt_id     0\n",
            "text          0\n",
            "content       0\n",
            "wording       0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train[['content','wording']].describe())\n",
        "print('\\nMissing values:\\n', train.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Awkk6INud7wg",
      "metadata": {
        "id": "Awkk6INud7wg"
      },
      "source": [
        "### Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "m-1APVWtd8NW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-1APVWtd8NW",
        "outputId": "b979d4e2-cc40-4f1c-fb88-60ba8dcc81cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                   0  \\\n",
            "text_combined_raw  Background \\r\\nThe Third Wave experiment took ...   \n",
            "text_proc          background third wave experiment took place cu...   \n",
            "\n",
            "                                                                   1  \n",
            "text_combined_raw  With one member trimming beef in a cannery, an...  \n",
            "text_proc          one member trimming beef cannery another worki...  \n"
          ]
        }
      ],
      "source": [
        "def clean_text(text, remove_stopwords=True):\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|\\S+@\\S+', ' ', text)\n",
        "    # mantener solo letras y espacios\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    # colapsar espacios\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    if remove_stopwords:\n",
        "        toks = [w for w in word_tokenize(text) if w not in STOPWORDS]\n",
        "        return ' '.join(toks)\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# función para crear campo combinado (prompt + resumen)\n",
        "def prepare_text_fields(df, prompts_df, text_col='text'):\n",
        "    prompts_small = prompts_df[['prompt_id', 'prompt_text']].drop_duplicates()\n",
        "    df = df.merge(prompts_small, on='prompt_id', how='left')\n",
        "    # concatenar prompt_text + summary text (espacio)\n",
        "    df['text_combined_raw'] = df['prompt_text'].fillna('') + ' ' + df[text_col].fillna('')\n",
        "    df['text_proc'] = df['text_combined_raw'].apply(clean_text)\n",
        "    return df\n",
        "\n",
        "# Prueba rápida (no sobreescribe aún)\n",
        "train_tmp = prepare_text_fields(train.copy(), prompts_train)\n",
        "print(train_tmp[['text_combined_raw','text_proc']].head(2).T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe9651f7",
      "metadata": {},
      "source": [
        "### Features simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fb6be6de",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    count         mean         std          min          25%  \\\n",
            "char_count         7165.0  2646.686113  400.987844  2049.000000  2344.000000   \n",
            "word_count         7165.0   377.495604   77.365926   285.000000   320.000000   \n",
            "sentence_count     7165.0     1.000000    0.000000     1.000000     1.000000   \n",
            "unique_word_ratio  7165.0     0.667301    0.040321     0.435626     0.641350   \n",
            "avg_word_len       7165.0     6.087425    0.447684     5.321881     5.456897   \n",
            "\n",
            "                           50%          75%          max  \n",
            "char_count         2555.000000  3009.000000  4899.000000  \n",
            "word_count          343.000000   466.000000   714.000000  \n",
            "sentence_count        1.000000     1.000000     1.000000  \n",
            "unique_word_ratio     0.666667     0.697068     0.755352  \n",
            "avg_word_len          6.221477     6.314448     6.857143  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "def add_basic_text_features(df, text_col='text_proc'):\n",
        "    df = df.copy()\n",
        "    df['char_count'] = df[text_col].astype(str).apply(len)\n",
        "    df['word_count'] = df[text_col].astype(str).apply(lambda x: len(x.split()))\n",
        "    df['sentence_count'] = df[text_col].astype(str).apply(lambda x: len(sent_tokenize(x)) if x.strip() else 0)\n",
        "    df['unique_word_ratio'] = df[text_col].astype(str).apply(lambda x: len(set(x.split())) / (len(x.split()) + 1e-9))\n",
        "    df['avg_word_len'] = df[text_col].astype(str).apply(lambda x: np.mean([len(w) for w in x.split()]) if x.split() else 0)\n",
        "    df['exclamation_count'] = df['text_combined_raw'].astype(str).apply(lambda x: x.count('!'))\n",
        "    return df\n",
        "\n",
        "# Ejemplo\n",
        "train_p = prepare_text_fields(train.copy(), prompts_train)\n",
        "train_p = add_basic_text_features(train_p)\n",
        "print(train_p[['char_count','word_count','sentence_count','unique_word_ratio','avg_word_len']].describe().T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a74fab",
      "metadata": {},
      "source": [
        "### TF-IDF y reducción"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9905ebdd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF vocab size: 20000\n",
            "SVD shape: (7165, 150)\n"
          ]
        }
      ],
      "source": [
        "def build_tfidf_svd(train_texts, max_features=30000, n_components=150):\n",
        "    tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=max_features)\n",
        "    X_tfidf = tfidf.fit_transform(train_texts)\n",
        "    if n_components and n_components > 0:\n",
        "        svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "        X_svd = svd.fit_transform(X_tfidf)\n",
        "    else:\n",
        "        svd = None\n",
        "        X_svd = X_tfidf  # sparse\n",
        "    return tfidf, svd, X_svd\n",
        "\n",
        "# Ejecución\n",
        "tfidf, svd, X_train_svd = build_tfidf_svd(train_p['text_proc'].fillna(''), max_features=20000, n_components=150)\n",
        "print('TF-IDF vocab size:', len(tfidf.vocabulary_))\n",
        "print('SVD shape:', X_train_svd.shape)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
